{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "# The training set just has blanks instead of 0s\n",
    "train_labels = pd.read_csv(\"../CheXpert-v1.0-small/train.csv\").fillna(0)\n",
    "train_labels[\"Path\"] = '../' + train_labels[\"Path\"]\n",
    "validation_labels = pd.read_csv('../CheXpert-v1.0-small/valid.csv')\n",
    "validation_labels[\"Path\"] = '../' + validation_labels[\"Path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out Lateral images.  We'll train two models -> one for lateral and one for frontal\n",
    "\n",
    "frontal_train_labels = train_labels[train_labels['Frontal/Lateral'] == 'Frontal']\n",
    "frontal_validation_labels = validation_labels[validation_labels['Frontal/Lateral'] == 'Frontal']\n",
    "\n",
    "# Filter out uncertains in the training dataset.  There are no uncertains in the validation dataset.\n",
    "frontal_train_labels = frontal_train_labels[frontal_train_labels[\"Lung Opacity\"] != -1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../CheXpert-v1.0-small/train/patient00001/stud...</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../CheXpert-v1.0-small/train/patient00002/stud...</td>\n",
       "      <td>Female</td>\n",
       "      <td>87</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../CheXpert-v1.0-small/train/patient00002/stud...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../CheXpert-v1.0-small/train/patient00003/stud...</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../CheXpert-v1.0-small/train/patient00004/stud...</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  ../CheXpert-v1.0-small/train/patient00001/stud...  Female   68   \n",
       "1  ../CheXpert-v1.0-small/train/patient00002/stud...  Female   87   \n",
       "2  ../CheXpert-v1.0-small/train/patient00002/stud...  Female   83   \n",
       "4  ../CheXpert-v1.0-small/train/patient00003/stud...    Male   41   \n",
       "5  ../CheXpert-v1.0-small/train/patient00004/stud...  Female   20   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         1.0                         0.0           0.0   \n",
       "1         Frontal    AP         0.0                         0.0          -1.0   \n",
       "2         Frontal    AP         0.0                         0.0           0.0   \n",
       "4         Frontal    AP         0.0                         0.0           0.0   \n",
       "5         Frontal    PA         1.0                         0.0           0.0   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "1           1.0          0.0   -1.0           -1.0        0.0         -1.0   \n",
       "2           1.0          0.0    0.0           -1.0        0.0          0.0   \n",
       "4           0.0          0.0    1.0            0.0        0.0          0.0   \n",
       "5           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0           0.0               0.0            0.0       0.0              1.0  \n",
       "1           0.0              -1.0            0.0       1.0              0.0  \n",
       "2           0.0               0.0            0.0       1.0              0.0  \n",
       "4           0.0               0.0            0.0       0.0              0.0  \n",
       "5           0.0               0.0            0.0       0.0              0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontal_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../CheXpert-v1.0-small/valid/patient64541/stud...</td>\n",
       "      <td>Male</td>\n",
       "      <td>73</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../CheXpert-v1.0-small/valid/patient64542/stud...</td>\n",
       "      <td>Male</td>\n",
       "      <td>70</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>PA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../CheXpert-v1.0-small/valid/patient64543/stud...</td>\n",
       "      <td>Male</td>\n",
       "      <td>85</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../CheXpert-v1.0-small/valid/patient64544/stud...</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../CheXpert-v1.0-small/valid/patient64545/stud...</td>\n",
       "      <td>Female</td>\n",
       "      <td>55</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  ../CheXpert-v1.0-small/valid/patient64541/stud...    Male   73   \n",
       "1  ../CheXpert-v1.0-small/valid/patient64542/stud...    Male   70   \n",
       "3  ../CheXpert-v1.0-small/valid/patient64543/stud...    Male   85   \n",
       "4  ../CheXpert-v1.0-small/valid/patient64544/stud...  Female   42   \n",
       "5  ../CheXpert-v1.0-small/valid/patient64545/stud...  Female   55   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         0.0                         1.0           1.0   \n",
       "1         Frontal    PA         0.0                         0.0           0.0   \n",
       "3         Frontal    AP         0.0                         1.0           0.0   \n",
       "4         Frontal    AP         1.0                         0.0           0.0   \n",
       "5         Frontal    AP         0.0                         1.0           0.0   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           1.0          0.0    0.0            0.0        0.0          0.0   \n",
       "1           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "3           1.0          0.0    1.0            0.0        0.0          0.0   \n",
       "4           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "5           1.0          0.0    0.0            0.0        0.0          1.0   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0           0.0               0.0            0.0       0.0              0.0  \n",
       "1           0.0               0.0            0.0       0.0              1.0  \n",
       "3           0.0               0.0            0.0       0.0              0.0  \n",
       "4           0.0               0.0            0.0       0.0              0.0  \n",
       "5           0.0               1.0            0.0       0.0              0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontal_validation_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.629365</td>\n",
       "      <td>0.090967</td>\n",
       "      <td>-0.005177</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.504893</td>\n",
       "      <td>0.031474</td>\n",
       "      <td>0.200149</td>\n",
       "      <td>-0.060596</td>\n",
       "      <td>-0.059390</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.080173</td>\n",
       "      <td>0.357596</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.036276</td>\n",
       "      <td>0.558115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.821530</td>\n",
       "      <td>0.287562</td>\n",
       "      <td>0.317897</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.203722</td>\n",
       "      <td>0.532046</td>\n",
       "      <td>0.433404</td>\n",
       "      <td>0.322723</td>\n",
       "      <td>0.553443</td>\n",
       "      <td>0.317387</td>\n",
       "      <td>0.569870</td>\n",
       "      <td>0.148507</td>\n",
       "      <td>0.200450</td>\n",
       "      <td>0.506179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Age     No Finding  Enlarged Cardiomediastinum  \\\n",
       "count  186596.000000  186596.000000               186596.000000   \n",
       "mean       60.629365       0.090967                   -0.005177   \n",
       "std        17.821530       0.287562                    0.317897   \n",
       "min         0.000000       0.000000                   -1.000000   \n",
       "25%        49.000000       0.000000                    0.000000   \n",
       "50%        62.000000       0.000000                    0.000000   \n",
       "75%        74.000000       0.000000                    0.000000   \n",
       "max        90.000000       1.000000                    1.000000   \n",
       "\n",
       "        Cardiomegaly   Lung Opacity    Lung Lesion          Edema  \\\n",
       "count  186596.000000  186596.000000  186596.000000  186596.000000   \n",
       "mean        0.087510       0.504893       0.031474       0.200149   \n",
       "std         0.386847       0.499977       0.203722       0.532046   \n",
       "min        -1.000000       0.000000      -1.000000      -1.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       1.000000       0.000000       0.000000   \n",
       "75%         0.000000       1.000000       0.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Consolidation      Pneumonia    Atelectasis   Pneumothorax  \\\n",
       "count  186596.000000  186596.000000  186596.000000  186596.000000   \n",
       "mean       -0.060596      -0.059390       0.006972       0.080173   \n",
       "std         0.433404       0.322723       0.553443       0.317387   \n",
       "min        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Pleural Effusion  Pleural Other       Fracture  Support Devices  \n",
       "count     186596.000000  186596.000000  186596.000000    186596.000000  \n",
       "mean           0.357596       0.003848       0.036276         0.558115  \n",
       "std            0.569870       0.148507       0.200450         0.506179  \n",
       "min           -1.000000      -1.000000      -1.000000        -1.000000  \n",
       "25%            0.000000       0.000000       0.000000         0.000000  \n",
       "50%            0.000000       0.000000       0.000000         1.000000  \n",
       "75%            1.000000       0.000000       0.000000         1.000000  \n",
       "max            1.000000       1.000000       1.000000         1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontal_train_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.00000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.00000</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.816832</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.519802</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>0.579208</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.336303</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500849</td>\n",
       "      <td>0.470184</td>\n",
       "      <td>0.494913</td>\n",
       "      <td>0.07036</td>\n",
       "      <td>0.406828</td>\n",
       "      <td>0.366038</td>\n",
       "      <td>0.195511</td>\n",
       "      <td>0.484349</td>\n",
       "      <td>0.183355</td>\n",
       "      <td>0.466397</td>\n",
       "      <td>0.07036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "count  202.000000  202.000000                  202.000000    202.000000   \n",
       "mean    60.816832    0.128713                    0.519802      0.326733   \n",
       "std     18.336303    0.335714                    0.500849      0.470184   \n",
       "min     18.000000    0.000000                    0.000000      0.000000   \n",
       "25%     48.250000    0.000000                    0.000000      0.000000   \n",
       "50%     62.500000    0.000000                    1.000000      0.000000   \n",
       "75%     74.750000    0.000000                    1.000000      1.000000   \n",
       "max     90.000000    1.000000                    1.000000      1.000000   \n",
       "\n",
       "       Lung Opacity  Lung Lesion       Edema  Consolidation   Pneumonia  \\\n",
       "count    202.000000    202.00000  202.000000     202.000000  202.000000   \n",
       "mean       0.579208      0.00495    0.207921       0.158416    0.039604   \n",
       "std        0.494913      0.07036    0.406828       0.366038    0.195511   \n",
       "min        0.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "25%        0.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "50%        1.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "75%        1.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "max        1.000000      1.00000    1.000000       1.000000    1.000000   \n",
       "\n",
       "       Atelectasis  Pneumothorax  Pleural Effusion  Pleural Other  Fracture  \\\n",
       "count   202.000000    202.000000        202.000000      202.00000     202.0   \n",
       "mean      0.371287      0.034653          0.316832        0.00495       0.0   \n",
       "std       0.484349      0.183355          0.466397        0.07036       0.0   \n",
       "min       0.000000      0.000000          0.000000        0.00000       0.0   \n",
       "25%       0.000000      0.000000          0.000000        0.00000       0.0   \n",
       "50%       0.000000      0.000000          0.000000        0.00000       0.0   \n",
       "75%       1.000000      0.000000          1.000000        0.00000       0.0   \n",
       "max       1.000000      1.000000          1.000000        1.00000       0.0   \n",
       "\n",
       "       Support Devices  \n",
       "count       202.000000  \n",
       "mean          0.490099  \n",
       "std           0.501144  \n",
       "min           0.000000  \n",
       "25%           0.000000  \n",
       "50%           0.000000  \n",
       "75%           1.000000  \n",
       "max           1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontal_validation_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 186596 validated image filenames.\n",
      "Found 202 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = datagen.flow_from_dataframe(dataframe=frontal_train_labels,\n",
    "                                            directory=\".\",\n",
    "                                            x_col=\"Path\",\n",
    "                                            y_col=['Lung Opacity'],\n",
    "                                            class_mode = \"raw\",\n",
    "                                            color_mode='grayscale',\n",
    "                                            target_size=(100, 100),\n",
    "                                            batch_size=32)\n",
    "validation_datagen = datagen.flow_from_dataframe(dataframe=frontal_validation_labels,\n",
    "                                            directory=\".\",\n",
    "                                            x_col=\"Path\",\n",
    "                                            y_col=['Lung Opacity'],\n",
    "                                            class_mode = \"raw\",\n",
    "                                            color_mode='grayscale',\n",
    "                                            target_size=(100, 100),\n",
    "                                            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics functions\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/120\n",
      " - 1811s - loss: 0.6411 - precision_m: 0.6152 - recall_m: 0.7421 - f1_m: 0.6656 - val_loss: 0.5065 - val_precision_m: 0.7590 - val_recall_m: 0.8269 - val_f1_m: 0.7857\n",
      "Epoch 17/120\n",
      " - 1800s - loss: 0.6413 - precision_m: 0.6131 - recall_m: 0.7544 - f1_m: 0.6696 - val_loss: 0.6078 - val_precision_m: 0.7849 - val_recall_m: 0.7255 - val_f1_m: 0.7475\n",
      "Epoch 18/120\n",
      " - 1807s - loss: 0.6401 - precision_m: 0.6172 - recall_m: 0.7360 - f1_m: 0.6644 - val_loss: 0.6242 - val_precision_m: 0.7777 - val_recall_m: 0.7876 - val_f1_m: 0.7761\n",
      "Epoch 19/120\n",
      " - 1802s - loss: 0.6384 - precision_m: 0.6183 - recall_m: 0.7523 - f1_m: 0.6722 - val_loss: 0.5579 - val_precision_m: 0.7089 - val_recall_m: 0.9192 - val_f1_m: 0.7962\n",
      "Epoch 20/120\n",
      " - 1803s - loss: 0.6384 - precision_m: 0.6164 - recall_m: 0.7516 - f1_m: 0.6706 - val_loss: 0.5794 - val_precision_m: 0.7826 - val_recall_m: 0.7413 - val_f1_m: 0.7552\n",
      "Epoch 21/120\n",
      " - 1807s - loss: 0.6373 - precision_m: 0.6200 - recall_m: 0.7472 - f1_m: 0.6709 - val_loss: 0.6326 - val_precision_m: 0.8307 - val_recall_m: 0.6347 - val_f1_m: 0.7109\n",
      "Epoch 22/120\n",
      " - 1801s - loss: 0.6382 - precision_m: 0.6206 - recall_m: 0.7389 - f1_m: 0.6678 - val_loss: 0.5693 - val_precision_m: 0.7799 - val_recall_m: 0.7872 - val_f1_m: 0.7775\n",
      "Epoch 23/120\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "initial_epoch = 15\n",
    "\n",
    "# L2 regularization uses the sum of the squares of the weights\n",
    "l2_regularization_constant = 0\n",
    "\n",
    "# 200x200 input\n",
    "# Input is kinda large, so we select larger filters for the first layer to decrease \n",
    "# size of feature maps (and hopefully speed up training).\n",
    "\n",
    "# Input: 100 x 100 x 1\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), use_bias=False))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 96 x 96 x 32\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 94 x 94 x 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 47 x 47 x 64\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 45 x 45 x 128\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 43 x 43 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 21 x 21 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 19 x 19 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 17 x 17 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 8 x 8 x 128\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Input: 8192 x 1024\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.002), loss='binary_crossentropy', metrics=[precision_m, recall_m, f1_m])\n",
    "\n",
    "mc = ModelCheckpoint('trial_5/weights{epoch:04d}.h5', \n",
    "                                     save_weights_only=True, period=5)\n",
    "\n",
    "# os.makedirs('trial_5')\n",
    "\n",
    "classifier.load_weights('trial_5/weights0015.h5')\n",
    "\n",
    "# too many epochs mean overfitting, not enough epochs mean underfitting\n",
    "classifier.fit_generator(\n",
    "    train_datagen,\n",
    "    steps_per_epoch=5280,\n",
    "    initial_epoch=initial_epoch,\n",
    "    epochs=120,\n",
    "    validation_data=validation_datagen,\n",
    "    validation_steps=800,\n",
    "    workers=4,\n",
    "    verbose=2,\n",
    "    callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1\n",
    "\n",
    "- 2 Convolutional Layers and 1 Fully-Connected Layer\n",
    "- No Regularization\n",
    "- Best Validation Loss: 0.53 (Epoch 7/35)\n",
    "- ~20 mins per epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(200, 200, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=128))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2\n",
    "\n",
    "- 6 convolutional layers with a max pooling layer every 2 convolutions\n",
    "- Little improvement in validation/training loss and accuracy over the first hour and a half of training\n",
    "- Best validation loss: 0.69 (last epoch for stopping)\n",
    "- ~20 mins/epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "\n",
    "# 200x200 input\n",
    "# Input is kinda large, so we select larger filters for the first layer to decrease \n",
    "# size of feature maps (and hopefully speed up training).\n",
    "\n",
    "# Input: 100 x 100 x 1\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu'))\n",
    "# Input: 96 x 96 x 32\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Input: 94 x 94 x 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 47 x 47 x 64\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 45 x 45 x 128\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 43 x 43 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 21 x 21 x 128\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 19 x 19 x 128\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 17 x 17 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 8 x 8 x 128\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Input: 8192 x 512\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 3\n",
    "\n",
    "- 2 convolutional layers with max pooling after every layer\n",
    "- 2 512-unit fully-connected layers\n",
    "- Best validation loss: 0.45 (Epoch 21/30)\n",
    "- ~16 mins per epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 4\n",
    "\n",
    "- Same as Trial 3 but with a batch normalization layer and 256-unit fully-connected layers\n",
    "- Best validation loss: 0.482 (Epoch 8/30)\n",
    "- ~16 mins per epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu', use_bias=False))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=256))\n",
    "classifier.add(Dense(activation=\"relu\", units=256))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 5\n",
    "\n",
    "- 6 convolutional layers and 2 fully-connected layers\n",
    "- Batch normalization after every convolutional layer\n",
    "- ReLU activation should be after pooling layer for performance (this was a mistake)\n",
    "- Dropout of 0.2 after every convolutional layer (this invalidates a random 20% of input units every training iteration).  This should help with overfitting\n",
    "- ~33 minutes per epoch\n",
    "- Best validation loss: 0.4183 (Epoch 35/65) \n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "\n",
    "# L2 regularization uses the sum of the squares of the weights\n",
    "l2_regularization_constant = 0\n",
    "\n",
    "# 200x200 input\n",
    "# Input is kinda large, so we select larger filters for the first layer to decrease \n",
    "# size of feature maps (and hopefully speed up training).\n",
    "\n",
    "# Input: 100 x 100 x 1\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), use_bias=False))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 96 x 96 x 32\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 94 x 94 x 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 47 x 47 x 64\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 45 x 45 x 128\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 43 x 43 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 21 x 21 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 19 x 19 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 17 x 17 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 8 x 8 x 128\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Input: 8192 x 1024\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
