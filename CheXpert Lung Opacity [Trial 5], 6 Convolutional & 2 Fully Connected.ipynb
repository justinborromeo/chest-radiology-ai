{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "# The training set just has blanks instead of 0s\n",
    "train_labels = pd.read_csv(\"CheXpert-v1.0-small/train.csv\").fillna(0)\n",
    "validation_labels = pd.read_csv('CheXpert-v1.0-small/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out Lateral images.  We'll train two models -> one for lateral and one for frontal\n",
    "\n",
    "frontal_train_labels = train_labels[train_labels['Frontal/Lateral'] == 'Frontal']\n",
    "frontal_validation_labels = validation_labels[validation_labels['Frontal/Lateral'] == 'Frontal']\n",
    "\n",
    "# Filter out uncertains in the training dataset.  There are no uncertains in the validation dataset.\n",
    "frontal_train_labels = frontal_train_labels[frontal_train_labels[\"Lung Opacity\"] != -1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_validation_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "      <td>186596.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.629365</td>\n",
       "      <td>0.090967</td>\n",
       "      <td>-0.005177</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.504893</td>\n",
       "      <td>0.031474</td>\n",
       "      <td>0.200149</td>\n",
       "      <td>-0.060596</td>\n",
       "      <td>-0.059390</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.080173</td>\n",
       "      <td>0.357596</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.036276</td>\n",
       "      <td>0.558115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.821530</td>\n",
       "      <td>0.287562</td>\n",
       "      <td>0.317897</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.203722</td>\n",
       "      <td>0.532046</td>\n",
       "      <td>0.433404</td>\n",
       "      <td>0.322723</td>\n",
       "      <td>0.553443</td>\n",
       "      <td>0.317387</td>\n",
       "      <td>0.569870</td>\n",
       "      <td>0.148507</td>\n",
       "      <td>0.200450</td>\n",
       "      <td>0.506179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Age     No Finding  Enlarged Cardiomediastinum  \\\n",
       "count  186596.000000  186596.000000               186596.000000   \n",
       "mean       60.629365       0.090967                   -0.005177   \n",
       "std        17.821530       0.287562                    0.317897   \n",
       "min         0.000000       0.000000                   -1.000000   \n",
       "25%        49.000000       0.000000                    0.000000   \n",
       "50%        62.000000       0.000000                    0.000000   \n",
       "75%        74.000000       0.000000                    0.000000   \n",
       "max        90.000000       1.000000                    1.000000   \n",
       "\n",
       "        Cardiomegaly   Lung Opacity    Lung Lesion          Edema  \\\n",
       "count  186596.000000  186596.000000  186596.000000  186596.000000   \n",
       "mean        0.087510       0.504893       0.031474       0.200149   \n",
       "std         0.386847       0.499977       0.203722       0.532046   \n",
       "min        -1.000000       0.000000      -1.000000      -1.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       1.000000       0.000000       0.000000   \n",
       "75%         0.000000       1.000000       0.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Consolidation      Pneumonia    Atelectasis   Pneumothorax  \\\n",
       "count  186596.000000  186596.000000  186596.000000  186596.000000   \n",
       "mean       -0.060596      -0.059390       0.006972       0.080173   \n",
       "std         0.433404       0.322723       0.553443       0.317387   \n",
       "min        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Pleural Effusion  Pleural Other       Fracture  Support Devices  \n",
       "count     186596.000000  186596.000000  186596.000000    186596.000000  \n",
       "mean           0.357596       0.003848       0.036276         0.558115  \n",
       "std            0.569870       0.148507       0.200450         0.506179  \n",
       "min           -1.000000      -1.000000      -1.000000        -1.000000  \n",
       "25%            0.000000       0.000000       0.000000         0.000000  \n",
       "50%            0.000000       0.000000       0.000000         1.000000  \n",
       "75%            1.000000       0.000000       0.000000         1.000000  \n",
       "max            1.000000       1.000000       1.000000         1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontal_train_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.00000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>202.00000</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.816832</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.519802</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>0.579208</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.336303</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.500849</td>\n",
       "      <td>0.470184</td>\n",
       "      <td>0.494913</td>\n",
       "      <td>0.07036</td>\n",
       "      <td>0.406828</td>\n",
       "      <td>0.366038</td>\n",
       "      <td>0.195511</td>\n",
       "      <td>0.484349</td>\n",
       "      <td>0.183355</td>\n",
       "      <td>0.466397</td>\n",
       "      <td>0.07036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "count  202.000000  202.000000                  202.000000    202.000000   \n",
       "mean    60.816832    0.128713                    0.519802      0.326733   \n",
       "std     18.336303    0.335714                    0.500849      0.470184   \n",
       "min     18.000000    0.000000                    0.000000      0.000000   \n",
       "25%     48.250000    0.000000                    0.000000      0.000000   \n",
       "50%     62.500000    0.000000                    1.000000      0.000000   \n",
       "75%     74.750000    0.000000                    1.000000      1.000000   \n",
       "max     90.000000    1.000000                    1.000000      1.000000   \n",
       "\n",
       "       Lung Opacity  Lung Lesion       Edema  Consolidation   Pneumonia  \\\n",
       "count    202.000000    202.00000  202.000000     202.000000  202.000000   \n",
       "mean       0.579208      0.00495    0.207921       0.158416    0.039604   \n",
       "std        0.494913      0.07036    0.406828       0.366038    0.195511   \n",
       "min        0.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "25%        0.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "50%        1.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "75%        1.000000      0.00000    0.000000       0.000000    0.000000   \n",
       "max        1.000000      1.00000    1.000000       1.000000    1.000000   \n",
       "\n",
       "       Atelectasis  Pneumothorax  Pleural Effusion  Pleural Other  Fracture  \\\n",
       "count   202.000000    202.000000        202.000000      202.00000     202.0   \n",
       "mean      0.371287      0.034653          0.316832        0.00495       0.0   \n",
       "std       0.484349      0.183355          0.466397        0.07036       0.0   \n",
       "min       0.000000      0.000000          0.000000        0.00000       0.0   \n",
       "25%       0.000000      0.000000          0.000000        0.00000       0.0   \n",
       "50%       0.000000      0.000000          0.000000        0.00000       0.0   \n",
       "75%       1.000000      0.000000          1.000000        0.00000       0.0   \n",
       "max       1.000000      1.000000          1.000000        1.00000       0.0   \n",
       "\n",
       "       Support Devices  \n",
       "count       202.000000  \n",
       "mean          0.490099  \n",
       "std           0.501144  \n",
       "min           0.000000  \n",
       "25%           0.000000  \n",
       "50%           0.000000  \n",
       "75%           1.000000  \n",
       "max           1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frontal_validation_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 186596 validated image filenames.\n",
      "Found 202 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = datagen.flow_from_dataframe(dataframe=frontal_train_labels,\n",
    "                                            directory=\".\",\n",
    "                                            x_col=\"Path\",\n",
    "                                            y_col=['Lung Opacity'],\n",
    "                                            class_mode = \"raw\",\n",
    "                                            color_mode='grayscale',\n",
    "                                            target_size=(100, 100),\n",
    "                                            batch_size=32)\n",
    "validation_datagen = datagen.flow_from_dataframe(dataframe=frontal_validation_labels,\n",
    "                                            directory=\".\",\n",
    "                                            x_col=\"Path\",\n",
    "                                            y_col=['Lung Opacity'],\n",
    "                                            class_mode = \"raw\",\n",
    "                                            color_mode='grayscale',\n",
    "                                            target_size=(100, 100),\n",
    "                                            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics functions\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      " - 1864s - loss: 0.7115 - precision_m: 0.5686 - recall_m: 0.7451 - f1_m: 0.6313 - val_loss: 0.7452 - val_precision_m: 0.7723 - val_recall_m: 0.1620 - val_f1_m: 0.2597\n",
      "Epoch 2/120\n",
      " - 1870s - loss: 0.6623 - precision_m: 0.5871 - recall_m: 0.7368 - f1_m: 0.6452 - val_loss: 0.6471 - val_precision_m: 0.6701 - val_recall_m: 0.8780 - val_f1_m: 0.7550\n",
      "Epoch 3/120\n",
      " - 1858s - loss: 0.6592 - precision_m: 0.5968 - recall_m: 0.7149 - f1_m: 0.6422 - val_loss: 0.6462 - val_precision_m: 0.7793 - val_recall_m: 0.3209 - val_f1_m: 0.4440\n",
      "Epoch 4/120\n",
      " - 1859s - loss: 0.6559 - precision_m: 0.5993 - recall_m: 0.7190 - f1_m: 0.6455 - val_loss: 0.6486 - val_precision_m: 0.5788 - val_recall_m: 1.0000 - val_f1_m: 0.7284\n",
      "Epoch 5/120\n",
      " - 1859s - loss: 0.6547 - precision_m: 0.5987 - recall_m: 0.7328 - f1_m: 0.6515 - val_loss: 0.6263 - val_precision_m: 0.8457 - val_recall_m: 0.5185 - val_f1_m: 0.6336\n",
      "Epoch 6/120\n",
      " - 1865s - loss: 0.6550 - precision_m: 0.6017 - recall_m: 0.7223 - f1_m: 0.6483 - val_loss: 0.6102 - val_precision_m: 0.5975 - val_recall_m: 0.9837 - val_f1_m: 0.7387\n",
      "Epoch 7/120\n",
      " - 1859s - loss: 0.6527 - precision_m: 0.6037 - recall_m: 0.7152 - f1_m: 0.6469 - val_loss: 0.6130 - val_precision_m: 0.8185 - val_recall_m: 0.5757 - val_f1_m: 0.6668\n",
      "Epoch 8/120\n",
      " - 1860s - loss: 0.6488 - precision_m: 0.6065 - recall_m: 0.7340 - f1_m: 0.6571 - val_loss: 0.5920 - val_precision_m: 0.7411 - val_recall_m: 0.8853 - val_f1_m: 0.8017\n",
      "Epoch 9/120\n",
      " - 1862s - loss: 0.6478 - precision_m: 0.6094 - recall_m: 0.7330 - f1_m: 0.6585 - val_loss: 0.5867 - val_precision_m: 0.8692 - val_recall_m: 0.5370 - val_f1_m: 0.6551\n",
      "Epoch 10/120\n",
      " - 1866s - loss: 0.6467 - precision_m: 0.6090 - recall_m: 0.7385 - f1_m: 0.6604 - val_loss: 0.5716 - val_precision_m: 0.7802 - val_recall_m: 0.7879 - val_f1_m: 0.7780\n",
      "Epoch 11/120\n",
      " - 1916s - loss: 0.6472 - precision_m: 0.6074 - recall_m: 0.7441 - f1_m: 0.6618 - val_loss: 0.6229 - val_precision_m: 0.7765 - val_recall_m: 0.4701 - val_f1_m: 0.5757\n",
      "Epoch 12/120\n",
      " - 1890s - loss: 0.6457 - precision_m: 0.6088 - recall_m: 0.7401 - f1_m: 0.6607 - val_loss: 0.5155 - val_precision_m: 0.8063 - val_recall_m: 0.6480 - val_f1_m: 0.7111\n",
      "Epoch 13/120\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# L2 regularization uses the sum of the squares of the weights\n",
    "l2_regularization_constant = 0\n",
    "\n",
    "# 200x200 input\n",
    "# Input is kinda large, so we select larger filters for the first layer to decrease \n",
    "# size of feature maps (and hopefully speed up training).\n",
    "\n",
    "# Input: 100 x 100 x 1\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), use_bias=False))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 96 x 96 x 32\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 94 x 94 x 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 47 x 47 x 64\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 45 x 45 x 128\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 43 x 43 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 21 x 21 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 19 x 19 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 17 x 17 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 8 x 8 x 128\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Input: 8192 x 1024\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.002), loss='binary_crossentropy', metrics=[precision_m, recall_m, f1_m])\n",
    "\n",
    "mc = ModelCheckpoint('trial_5/weights{epoch:04d}.h5', \n",
    "                                     save_weights_only=True, period=5)\n",
    "\n",
    "os.makedirs('trial_5')\n",
    "\n",
    "# too many epochs mean overfitting, not enough epochs mean underfitting\n",
    "classifier.fit_generator(\n",
    "    train_datagen,\n",
    "    steps_per_epoch=5280,\n",
    "    epochs=120,\n",
    "    validation_data=validation_datagen,\n",
    "    validation_steps=800,\n",
    "    workers=4,\n",
    "    verbose=2,\n",
    "    callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1\n",
    "\n",
    "- 2 Convolutional Layers and 1 Fully-Connected Layer\n",
    "- No Regularization\n",
    "- Best Validation Loss: 0.53 (Epoch 7/35)\n",
    "- ~20 mins per epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(200, 200, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=128))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2\n",
    "\n",
    "- 6 convolutional layers with a max pooling layer every 2 convolutions\n",
    "- Little improvement in validation/training loss and accuracy over the first hour and a half of training\n",
    "- Best validation loss: 0.69 (last epoch for stopping)\n",
    "- ~20 mins/epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "\n",
    "# 200x200 input\n",
    "# Input is kinda large, so we select larger filters for the first layer to decrease \n",
    "# size of feature maps (and hopefully speed up training).\n",
    "\n",
    "# Input: 100 x 100 x 1\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu'))\n",
    "# Input: 96 x 96 x 32\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Input: 94 x 94 x 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 47 x 47 x 64\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 45 x 45 x 128\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 43 x 43 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 21 x 21 x 128\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 19 x 19 x 128\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Input: 17 x 17 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 8 x 8 x 128\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Input: 8192 x 512\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 3\n",
    "\n",
    "- 2 convolutional layers with max pooling after every layer\n",
    "- 2 512-unit fully-connected layers\n",
    "- Best validation loss: 0.45 (Epoch 21/30)\n",
    "- ~16 mins per epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"relu\", units=512))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 4\n",
    "\n",
    "- Same as Trial 3 but with a batch normalization layer and 256-unit fully-connected layers\n",
    "- Best validation loss: 0.482 (Epoch 8/30)\n",
    "- ~16 mins per epoch\n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu', use_bias=False))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=256))\n",
    "classifier.add(Dense(activation=\"relu\", units=256))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 5\n",
    "\n",
    "- 6 convolutional layers and 2 fully-connected layers\n",
    "- Batch normalization after every convolutional layer\n",
    "- ReLU activation should be after pooling layer for performance (this was a mistake)\n",
    "- Dropout of 0.2 after every convolutional layer (this invalidates a random 20% of input units every training iteration).  This should help with overfitting\n",
    "- ~33 minutes per epoch\n",
    "- Best validation loss: 0.4183 (Epoch 35/65) \n",
    "\n",
    "```\n",
    "classifier = Sequential()\n",
    "\n",
    "# L2 regularization uses the sum of the squares of the weights\n",
    "l2_regularization_constant = 0\n",
    "\n",
    "# 200x200 input\n",
    "# Input is kinda large, so we select larger filters for the first layer to decrease \n",
    "# size of feature maps (and hopefully speed up training).\n",
    "\n",
    "# Input: 100 x 100 x 1\n",
    "classifier.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), use_bias=False))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 96 x 96 x 32\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 94 x 94 x 64\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 47 x 47 x 64\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 45 x 45 x 128\n",
    "classifier.add(Conv2D(64, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 43 x 43 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 21 x 21 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 19 x 19 x 128\n",
    "classifier.add(Conv2D(128, (3, 3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(.2))\n",
    "# Input: 17 x 17 x 128\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Input: 8 x 8 x 128\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Input: 8192 x 1024\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"relu\", units=1024))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
